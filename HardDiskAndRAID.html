<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
先来扯淡吧，圣诞节前几天一直在看一些论文，磁盘相关的，今天索性一次性写完，不然看了又忘记了。  
##1. 硬盘
　　硬盘的基础知识就不赘述了，什么磁头磁道扇区等等啦，看下图一目了然，不了然的话自己看书去。  

　　　　![磁盘](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/sectors-and-clusters1.png)    
　　然后有一点需要明确一下，那就是磁盘保证对于扇区的更新操作都是**原子**的。每次写扇区要么全写进去了要么全没写进去。  
　　下面写一些磁盘的调度算法吧。调度嘛，就是说如果有多个请求，磁盘怎么依次处理这些请求。有一个大的准则，持续时间短的请求先执行(**shortest job first**)，这个也很好理解，毕竟使得所有请求的平均等待时间最小。但是，这个很难做到，因为很难预先知道一个请求执行所需要的时间。于是乎就有了下面的近似算法了:

- **Shortest Seek Time First(SSTF)**  
　　这个就是，下一次执行的磁盘命令是选择最近磁道上的请求命令，但是这个也很难做到，因为操作系统根本看不见磁道啊，它也不知道磁盘的空间几何分布，所以实际上有个替代的，**nearst-block-first(NBF)**，物理地址最相近的最先执行。  
　　听起来这个**NBF调度**应该是不错的，但是有一个特殊情况，就是说距离当前磁头位置比较远的那个磁盘请求可能“永远”也没法被调度到。这就是调度中经常需要考虑的“饥饿”问题。  


2. **Elevator**  
　　这就是升降机算法了。将当前所有的磁盘请求命令按照**NBF**调度依次执行，执行过程中收到的磁盘请求暂时不处理。这就解决了上面的饥饿问题了。

3. **Shortest Positioning Time First(SPTF)**
　　这个呢，是考虑到了实际的磁盘寻址时间，因为你物理地址相近不代表磁头寻址所需时间短，还得考虑一个磁盘转向问题。这是因为在现在的磁盘中，一般而言，寻址所需的磁臂移动时间和磁盘转动时间是相当的，于是乎，才需要考虑磁盘转向。当然了，关于磁盘转向这些，交给OS来做就太细致繁琐了。所以实际上来操作的时候：  
　　首先由OS按照自己的调度算法给出几个候选人，然后交由磁盘自己的调度算法（**SPTF**）来具体调度；其次，磁盘调度还会做**I/O merging**操作，比如说有两个请求分别请求两个磁盘块，而这两个磁盘块之间是相邻的，于是乎就可以将这两个磁盘请求合并；最后，磁盘也并不是说只要有请求就不会休息，事实上，可能稍微等等会更好(**Anticipatory disk scheduling**)，当然了，这些设计都需要非常注意技巧，不是那么简单的，这里有个概念就可以了。

##2.  廉价磁盘冗余阵列(RAIDs)  
 　　**RAID**的英文全称是**Redundant Arrays of Inexpensive Disks**。磁盘阵列就是将几个磁盘连接在一起，用户可以并行访问这些磁盘，从而提高效率。而冗余阵列这个设计的主要目的就是在于通过提供某种形式的冗余来增强阵列的安全性，不至于说很多磁盘只要有一个坏了就会对数据造成损失。  
　　直接来吧，下面是**RAID0,RAID1,RAID4,RAID5**： 

 - **RAID0**  
  这个最简单了，原始的阵列，压根没有冗余，所以任意一个磁盘坏了数据就会出问题，直接上图了。图中的A,B,C,D等等指的是磁盘块，下同。  
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid-0.png)  

 - **RAID1**   
这个呢，其实就是通常意义上的镜像，分为两种，下面这个是**RAID10**，
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid10-6disks.png)    
这个是**RAID01**，  
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid01-6disks.png)  
看得出来区别吧，镜像的设计比较可靠，至少来说一个磁盘坏了没有什么关系，而最理想的情况下一半数目的磁盘坏了都没事。  

 - **RAID4**  
  先直接看图吧：  
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid4.png)    
哈哈，多了一个校验磁盘，这个盘的数据是怎么来的呢？前三个磁盘的数据按照 位异或 的操作就可以计算出来校验磁盘的数据了。这样有个好处，就是说任一磁盘损坏了，我们都可以利用剩下的三个磁盘通过 位异或 操作来恢复，厉害吧。但是这样设计有个缺点，每次写数据的时候需要同时也更新对应的校验磁盘中的数据，而言将校验块统一放置于校验磁盘中，也就意味着每次写操作都需要访问校验磁盘，如此一来，写操作的并行就不存在了，于是，聪明的人民设计了**RAID5**。  

 - **RAID5**  
  先直接看图吧：  
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid-5.png)    

　　下面来说说这几个级别的RAID的读写效率吧，抗损坏能力之前已经说了。这个吧，说起来太复杂了，我会在后面留下参考资料，这里就直接给出结论了，对照着看吧。
　　　　![](http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/RAID.jpg)


##3.  参考文献 

>1. http://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf  
>2. http://pages.cs.wisc.edu/~remzi/OSTEP/file-disks.pdf  
>3. http://www.thegeekstuff.com/2010/08/raid-levels-tutorial/
>4. http://www.thegeekstuff.com/2011/10/raid10-vs-raid01/  
>5. http://www.thegeekstuff.com/2011/11/raid2-raid3-raid4-raid6/
<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>先来扯淡吧，圣诞节前几天一直在看一些论文，磁盘相关的，今天索性一次性写完，不然看了又忘记了。  </p>

<h2 id="1">1. 硬盘</h2>

<p>　　硬盘的基础知识就不赘述了，什么磁头磁道扇区等等啦，看下图一目了然，不了然的话自己看书去。  </p>

<p>　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/sectors-and-clusters1.png" alt="磁盘" title=""> <br>
　　然后有一点需要明确一下，那就是磁盘保证对于扇区的更新操作都是<strong>原子</strong>的。每次写扇区要么全写进去了要么全没写进去。 <br>
　　下面写一些磁盘的调度算法吧。调度嘛，就是说如果有多个请求，磁盘怎么依次处理这些请求。有一个大的准则，持续时间短的请求先执行(<strong>shortest job first</strong>)，这个也很好理解，毕竟使得所有请求的平均等待时间最小。但是，这个很难做到，因为很难预先知道一个请求执行所需要的时间。于是乎就有了下面的近似算法了:</p>

<ul>
<li><p><strong>Shortest Seek Time First(SSTF)</strong> <br>
　　这个就是，下一次执行的磁盘命令是选择最近磁道上的请求命令，但是这个也很难做到，因为操作系统根本看不见磁道啊，它也不知道磁盘的空间几何分布，所以实际上有个替代的，<strong>nearst-block-first(NBF)</strong>，物理地址最相近的最先执行。 <br>
　　听起来这个<strong>NBF调度</strong>应该是不错的，但是有一个特殊情况，就是说距离当前磁头位置比较远的那个磁盘请求可能“永远”也没法被调度到。这就是调度中经常需要考虑的“饥饿”问题。  </p></li>
<li><p><strong>Elevator</strong> <br>
　　这就是升降机算法了。将当前所有的磁盘请求命令按照<strong>NBF</strong>调度依次执行，执行过程中收到的磁盘请求暂时不处理。这就解决了上面的饥饿问题了。</p></li>
<li><p><strong>Shortest Positioning Time First(SPTF)</strong>
　　这个呢，是考虑到了实际的磁盘寻址时间，因为你物理地址相近不代表磁头寻址所需时间短，还得考虑一个磁盘转向问题。这是因为在现在的磁盘中，一般而言，寻址所需的磁臂移动时间和磁盘转动时间是相当的，于是乎，才需要考虑磁盘转向。当然了，关于磁盘转向这些，交给OS来做就太细致繁琐了。所以实际上来操作的时候： <br>
　　首先由OS按照自己的调度算法给出几个候选人，然后交由磁盘自己的调度算法（<strong>SPTF</strong>）来具体调度；其次，磁盘调度还会做<strong>I/O merging</strong>操作，比如说有两个请求分别请求两个磁盘块，而这两个磁盘块之间是相邻的，于是乎就可以将这两个磁盘请求合并；最后，磁盘也并不是说只要有请求就不会休息，事实上，可能稍微等等会更好(<strong>Anticipatory disk scheduling</strong>)，当然了，这些设计都需要非常注意技巧，不是那么简单的，这里有个概念就可以了。</p></li>
</ul>

<h2 id="2raids">2.  廉价磁盘冗余阵列(RAIDs)</h2>

<p>　　<strong>RAID</strong>的英文全称是<strong>Redundant Arrays of Inexpensive Disks</strong>。磁盘阵列就是将几个磁盘连接在一起，用户可以并行访问这些磁盘，从而提高效率。而冗余阵列这个设计的主要目的就是在于通过提供某种形式的冗余来增强阵列的安全性，不至于说很多磁盘只要有一个坏了就会对数据造成损失。 <br>
　　直接来吧，下面是<strong>RAID0,RAID1,RAID4,RAID5</strong>： </p>

<ul>
<li><p><strong>RAID0</strong> <br>
这个最简单了，原始的阵列，压根没有冗余，所以任意一个磁盘坏了数据就会出问题，直接上图了。图中的A,B,C,D等等指的是磁盘块，下同。 <br>
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid-0.png" alt="" title="">  </p></li>
<li><p><strong>RAID1</strong> <br>
这个呢，其实就是通常意义上的镜像，分为两种，下面这个是<strong>RAID10</strong>，
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid10-6disks.png" alt="" title=""> <br>
这个是<strong>RAID01</strong>， <br>
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid01-6disks.png" alt="" title=""> <br>
看得出来区别吧，镜像的设计比较可靠，至少来说一个磁盘坏了没有什么关系，而最理想的情况下一半数目的磁盘坏了都没事。  </p></li>
<li><p><strong>RAID4</strong> <br>
先直接看图吧： <br>
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid4.png" alt="" title=""> <br>
哈哈，多了一个校验磁盘，这个盘的数据是怎么来的呢？前三个磁盘的数据按照 位异或 的操作就可以计算出来校验磁盘的数据了。这样有个好处，就是说任一磁盘损坏了，我们都可以利用剩下的三个磁盘通过 位异或 操作来恢复，厉害吧。但是这样设计有个缺点，每次写数据的时候需要同时也更新对应的校验磁盘中的数据，而言将校验块统一放置于校验磁盘中，也就意味着每次写操作都需要访问校验磁盘，如此一来，写操作的并行就不存在了，于是，聪明的人民设计了<strong>RAID5</strong>。  </p></li>
<li><p><strong>RAID5</strong> <br>
先直接看图吧： <br>
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/raid-5.png" alt="" title="">    </p>

<p>　　下面来说说这几个级别的RAID的读写效率吧，抗损坏能力之前已经说了。这个吧，说起来太复杂了，我会在后面留下参考资料，这里就直接给出结论了，对照着看吧。
　　　　<img src="http://7te99v.com1.z0.glb.clouddn.com/@/blog/HardDiskAndRAID/RAID.jpg" alt="" title=""></p></li>
</ul>

<h2 id="3">3.  参考文献</h2>

<blockquote>
  <ol>
  <li>http://pages.cs.wisc.edu/~remzi/OSTEP/file-raid.pdf  </li>
  <li>http://pages.cs.wisc.edu/~remzi/OSTEP/file-disks.pdf  </li>
  <li>http://www.thegeekstuff.com/2010/08/raid-levels-tutorial/</li>
  <li>http://www.thegeekstuff.com/2011/10/raid10-vs-raid01/  </li>
  <li>http://www.thegeekstuff.com/2011/11/raid2-raid3-raid4-raid6/</li>
  </ol>
</blockquote>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "HardDiskAndRAID.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
</body>
</html>
